#!/bin/bash

# Define model configurations: "model_type model_name epochs learning_rate PDE config_file"
models=(
    "PINN 2D_PINNmodel 10000 0.01 PDE1 0 500 Config/params.yml "
    "PINN 2D_PINNmodel 10000 0.01 PDE1 0 500 Config/AWparams.yml "
    "PINN 2D_PINNmodel 10000 0.01 PDE1 16 500 Config/ACparams.yml"
    "PINN 2D_PINNmodel 10000 0.01 PDE1 16 500 Config/ACAWparams.yml"
)

# Create a results directory if it doesn't exist
mkdir -p benchmark_results

# Start benchmarking
echo "Starting Benchmarking Process..."

for model in "${models[@]}"; do
    # Read values into variables
    set -- $model
    model_type=$1
    model_name=$2
    epochs=$3
    lr=$4
    pde=$5
    num_points=$6
    update_rate=$7
    config_file=$8
    save_model=$9

    # Create log file for each model
    log_file="benchmark_results/${model_name}_${config_file}_log.txt"

    echo "---------------------------------------------"
    echo "Running benchmark for: $model_name with $config_file"
    echo "Model Type: $model_type | Epochs: $epochs | LR: $lr | PDE: $pde | num_points: $num_points | update_rate: $update_rate | Config: $config_file | Save Model: $save_model"
    echo "---------------------------------------------"

    # Start timing the execution
    start_time=$(date +%s)

    # Run the Python script with the parameters
    python main.py --model_type "$model_type" --model_name "$model_name" --epochs "$epochs" --lr "$lr" --PDE "$pde" --AC "$num_points" --update_rate "$update_rate" --config "$config_file" --save_model "$save_model" > "$log_file" 2>&1

    # Calculate execution time
    end_time=$(date +%s)
    runtime=$((end_time - start_time))

    echo "Finished benchmarking for $model_name with $config_file in $runtime seconds"
    echo "Results saved in $log_file"
    echo "---------------------------------------------"
done

echo "âœ… Benchmarking Completed! All results are in the 'benchmark_results' folder."
